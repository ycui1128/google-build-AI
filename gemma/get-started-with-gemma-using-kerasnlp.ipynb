{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8968e4cc",
   "metadata": {
    "id": "4qxv4Sn9b8CE",
    "papermill": {
     "duration": 0.008847,
     "end_time": "2024-02-21T11:44:31.969668",
     "exception": false,
     "start_time": "2024-02-21T11:44:31.960821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Copyright 2024 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed56cc85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:44:31.989638Z",
     "iopub.status.busy": "2024-02-21T11:44:31.988602Z",
     "iopub.status.idle": "2024-02-21T11:44:31.995001Z",
     "shell.execute_reply": "2024-02-21T11:44:31.994101Z"
    },
    "papermill": {
     "duration": 0.01957,
     "end_time": "2024-02-21T11:44:31.997634",
     "exception": false,
     "start_time": "2024-02-21T11:44:31.978064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e89cb",
   "metadata": {
    "papermill": {
     "duration": 0.008082,
     "end_time": "2024-02-21T11:44:32.016845",
     "exception": false,
     "start_time": "2024-02-21T11:44:32.008763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/get_started\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
    "  </td>\n",
    "    <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/get_started.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/get_started.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74435dbf",
   "metadata": {
    "id": "PXNm5_p_oxMF",
    "papermill": {
     "duration": 0.0079,
     "end_time": "2024-02-21T11:44:32.033193",
     "exception": false,
     "start_time": "2024-02-21T11:44:32.025293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get started with Gemma using KerasNLP\n",
    "\n",
    "This tutorial shows you how to get started with Gemma using [KerasNLP](https://keras.io/keras_nlp/). Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models. KerasNLP is a collection of natural language processing (NLP) models implemented in [Keras](https://keras.io/) and runnable on JAX, PyTorch, and TensorFlow.\n",
    "\n",
    "In this tutorial, you'll use Gemma to generate text responses to several prompts. If you're new to Keras, you might want to read [Getting started with Keras](https://keras.io/getting_started/) before you begin, but you don't have to. You'll learn more about Keras as you work through this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee157b",
   "metadata": {
    "id": "mERVCCsGUPIJ",
    "papermill": {
     "duration": 0.007947,
     "end_time": "2024-02-21T11:44:32.049337",
     "exception": false,
     "start_time": "2024-02-21T11:44:32.041390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733bd525",
   "metadata": {
    "id": "QQ6W7NzRe1VM",
    "papermill": {
     "duration": 0.007948,
     "end_time": "2024-02-21T11:44:32.065863",
     "exception": false,
     "start_time": "2024-02-21T11:44:32.057915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Gemma setup\n",
    "\n",
    "To complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
    "\n",
    "Gemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n",
    "\n",
    "- Sign in or register at [kaggle.com](https://www.kaggle.com)\n",
    "- Open the [Gemma model card](https://www.kaggle.com/models/google/gemma) and select _\"Request Access\"_\n",
    "- Complete the consent form and accept the terms and conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073e0ec",
   "metadata": {
    "id": "z9oy3QUmXtSd",
    "papermill": {
     "duration": 0.007854,
     "end_time": "2024-02-21T11:44:32.082004",
     "exception": false,
     "start_time": "2024-02-21T11:44:32.074150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install dependencies\n",
    "\n",
    "Install Keras and KerasNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4011eb31",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-02-21T11:44:32.100909Z",
     "iopub.status.busy": "2024-02-21T11:44:32.100170Z",
     "iopub.status.idle": "2024-02-21T11:45:09.673414Z",
     "shell.execute_reply": "2024-02-21T11:45:09.671602Z"
    },
    "id": "UcGLzDeQ8NwN",
    "papermill": {
     "duration": 37.587148,
     "end_time": "2024-02-21T11:45:09.677425",
     "exception": false,
     "start_time": "2024-02-21T11:44:32.090277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\r\n",
      "tensorflowjs 4.16.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048245d",
   "metadata": {
    "id": "FX47AUYrXwLK",
    "papermill": {
     "duration": 0.008416,
     "end_time": "2024-02-21T11:45:09.695117",
     "exception": false,
     "start_time": "2024-02-21T11:45:09.686701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import packages\n",
    "\n",
    "Import Keras and KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0faf8b58",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-02-21T11:45:09.716855Z",
     "iopub.status.busy": "2024-02-21T11:45:09.716140Z",
     "iopub.status.idle": "2024-02-21T11:45:28.744453Z",
     "shell.execute_reply": "2024-02-21T11:45:28.743143Z"
    },
    "id": "ww83zI9ToPso",
    "papermill": {
     "duration": 19.043566,
     "end_time": "2024-02-21T11:45:28.748299",
     "exception": false,
     "start_time": "2024-02-21T11:45:09.704733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 11:45:12.498812: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-21 11:45:12.498986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-21 11:45:12.708308: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a085bfe",
   "metadata": {
    "id": "Pm5cVOFt5YvZ",
    "papermill": {
     "duration": 0.009132,
     "end_time": "2024-02-21T11:45:28.768116",
     "exception": false,
     "start_time": "2024-02-21T11:45:28.758984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Select a backend\n",
    "\n",
    "Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. [Keras 3](https://keras.io/keras_3) lets you choose the backend: TensorFlow, JAX, or PyTorch. All three will work for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3802bf9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:45:28.790499Z",
     "iopub.status.busy": "2024-02-21T11:45:28.789575Z",
     "iopub.status.idle": "2024-02-21T11:45:28.795937Z",
     "shell.execute_reply": "2024-02-21T11:45:28.794180Z"
    },
    "id": "7rS7ryTs5wjf",
    "papermill": {
     "duration": 0.021037,
     "end_time": "2024-02-21T11:45:28.798903",
     "exception": false,
     "start_time": "2024-02-21T11:45:28.777866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6a79a",
   "metadata": {
    "id": "ZsxDCbLN555T",
    "papermill": {
     "duration": 0.008299,
     "end_time": "2024-02-21T11:45:28.815848",
     "exception": false,
     "start_time": "2024-02-21T11:45:28.807549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create a model\n",
    "\n",
    "KerasNLP provides implementations of many popular [model architectures](https://keras.io/api/keras_nlp/models/). In this tutorial, you'll create a model using `GemmaCausalLM`, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.\n",
    "\n",
    "Create the model using the `from_preset` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd6ac92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:45:28.835200Z",
     "iopub.status.busy": "2024-02-21T11:45:28.834703Z",
     "iopub.status.idle": "2024-02-21T11:47:13.074305Z",
     "shell.execute_reply": "2024-02-21T11:47:13.072563Z"
    },
    "id": "yygIK9DEIldp",
    "papermill": {
     "duration": 104.253654,
     "end_time": "2024-02-21T11:47:13.077985",
     "exception": false,
     "start_time": "2024-02-21T11:45:28.824331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ac431",
   "metadata": {
    "id": "XrAWvsU6pI0E",
    "papermill": {
     "duration": 0.009558,
     "end_time": "2024-02-21T11:47:13.098335",
     "exception": false,
     "start_time": "2024-02-21T11:47:13.088777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`from_preset` instantiates the model from a preset architecture and weights. In the code above, the string `\"gemma_2b_en\"` specifies the preset architecture: a Gemma model with 2 billion parameters. (A Gemma model with 7 billion parameters is also available. To run the larger model in Colab, you need access to the premium GPUs available in paid plans. Alternatively, you can perform [distributed tuning on a Gemma 7B model](https://ai.google.dev/gemma/docs/distributed_tuning) on Kaggle or Google Cloud.)\n",
    "\n",
    "Use `summary` to get more info about the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f28bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:47:13.121134Z",
     "iopub.status.busy": "2024-02-21T11:47:13.120701Z",
     "iopub.status.idle": "2024-02-21T11:47:13.175217Z",
     "shell.execute_reply": "2024-02-21T11:47:13.173752Z"
    },
    "id": "e5nEbTdApL7W",
    "papermill": {
     "duration": 0.069378,
     "end_time": "2024-02-21T11:47:13.178133",
     "exception": false,
     "start_time": "2024-02-21T11:47:13.108755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55034e",
   "metadata": {
    "id": "81KHdRYOrWYm",
    "papermill": {
     "duration": 0.011872,
     "end_time": "2024-02-21T11:47:13.201075",
     "exception": false,
     "start_time": "2024-02-21T11:47:13.189203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see from the summary, the model has 2.5 billion trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb6c80",
   "metadata": {
    "id": "FOBW7piN5-sl",
    "papermill": {
     "duration": 0.010844,
     "end_time": "2024-02-21T11:47:13.223620",
     "exception": false,
     "start_time": "2024-02-21T11:47:13.212776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate text\n",
    "\n",
    "Now it's time to generate some text! The model has a `generate` method that generates text based on a prompt. The optional `max_length` argument specifies the maximum length of the generated sequence.\n",
    "\n",
    "Try it out with the prompt `\"What is the meaning of life?\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3c6fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:47:13.248989Z",
     "iopub.status.busy": "2024-02-21T11:47:13.248530Z",
     "iopub.status.idle": "2024-02-21T11:50:08.031207Z",
     "shell.execute_reply": "2024-02-21T11:50:08.028981Z"
    },
    "id": "aae5GHrdpj2_",
    "papermill": {
     "duration": 174.80553,
     "end_time": "2024-02-21T11:50:08.040827",
     "exception": false,
     "start_time": "2024-02-21T11:47:13.235297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708516065.502543      18 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-02-21 11:47:45.553925: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the meaning of life?\\n\\nThe question is one of the most important questions in the world.\\n\\nIt’s the question that has been asked by philosophers, theologians, and scientists for centuries.\\n\\nAnd it’s the question that has been asked by people who are looking for answers to their own lives'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_lm.generate(\"What is the meaning of life?\", max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c653e71",
   "metadata": {
    "id": "qH0eFH_DvYwM",
    "papermill": {
     "duration": 0.01063,
     "end_time": "2024-02-21T11:50:08.063212",
     "exception": false,
     "start_time": "2024-02-21T11:50:08.052582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Try calling `generate` again with a different prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6524b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:50:08.089056Z",
     "iopub.status.busy": "2024-02-21T11:50:08.088467Z",
     "iopub.status.idle": "2024-02-21T11:52:51.709311Z",
     "shell.execute_reply": "2024-02-21T11:52:51.707725Z"
    },
    "id": "VEyTnnNGvgGG",
    "papermill": {
     "duration": 163.648371,
     "end_time": "2024-02-21T11:52:51.722424",
     "exception": false,
     "start_time": "2024-02-21T11:50:08.074053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How does the brain work?\\n\\nThe brain is the most complex organ in the human body. It is responsible for controlling all of the body’s functions, including breathing, heart rate, digestion, and more. The brain is also responsible for thinking, feeling, and making decisions.\\n\\nThe brain is made up'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_lm.generate(\"How does the brain work?\", max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a71563",
   "metadata": {
    "id": "vVlCnY7Gvm7U",
    "papermill": {
     "duration": 0.013497,
     "end_time": "2024-02-21T11:52:51.747933",
     "exception": false,
     "start_time": "2024-02-21T11:52:51.734436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you're running on JAX or TensorFlow backends, you'll notice that the second `generate` call returns nearly instantly. This is because each call to `generate` for a given batch size and `max_length` is compiled with XLA. The first run is expensive, but subsequent runs are much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1e9ba",
   "metadata": {
    "id": "mw5XkiHU11Ft",
    "papermill": {
     "duration": 0.011949,
     "end_time": "2024-02-21T11:52:51.775627",
     "exception": false,
     "start_time": "2024-02-21T11:52:51.763678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can also provide batched prompts using a list as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa45a892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:52:51.807381Z",
     "iopub.status.busy": "2024-02-21T11:52:51.805982Z",
     "iopub.status.idle": "2024-02-21T11:58:38.518837Z",
     "shell.execute_reply": "2024-02-21T11:58:38.517507Z"
    },
    "id": "xV6vs8_C2BGt",
    "papermill": {
     "duration": 346.74814,
     "end_time": "2024-02-21T11:58:38.537247",
     "exception": false,
     "start_time": "2024-02-21T11:52:51.789107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the meaning of life?\\n\\nThe question is one of the most important questions in the world.\\n\\nIt’s the question that has been asked by philosophers, theologians, and scientists for centuries.\\n\\nAnd it’s the question that has been asked by people who are looking for answers to their own lives',\n",
       " 'How does the brain work?\\n\\nThe brain is the most complex organ in the human body. It is responsible for controlling all of the body’s functions, including breathing, heart rate, digestion, and more. The brain is also responsible for thinking, feeling, and making decisions.\\n\\nThe brain is made up']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_lm.generate(\n",
    "    [\"What is the meaning of life?\",\n",
    "     \"How does the brain work?\"],\n",
    "    max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a831f",
   "metadata": {
    "id": "MaVWoSpo3XyY",
    "papermill": {
     "duration": 0.012864,
     "end_time": "2024-02-21T11:58:38.562028",
     "exception": false,
     "start_time": "2024-02-21T11:58:38.549164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Optional: Try a different sampler\n",
    "\n",
    "You can control the generation strategy for `GemmaCausalLM` by setting the `sampler` argument on `compile()`. By default, [`\"greedy\"`](https://keras.io/api/keras_nlp/samplers/greedy_sampler/#greedysampler-class) sampling will be used.\n",
    "\n",
    "As an experiment, try setting a [`\"top_k\"`](https://keras.io/api/keras_nlp/samplers/top_k_sampler/) strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd7d7914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:58:38.589742Z",
     "iopub.status.busy": "2024-02-21T11:58:38.589089Z",
     "iopub.status.idle": "2024-02-21T12:01:37.401889Z",
     "shell.execute_reply": "2024-02-21T12:01:37.400607Z"
    },
    "id": "mx55VQpN4DAK",
    "papermill": {
     "duration": 178.839602,
     "end_time": "2024-02-21T12:01:37.413797",
     "exception": false,
     "start_time": "2024-02-21T11:58:38.574195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the meaning of life? That is a question that has been asked for centuries and has yet to be answered. However, there are some people who believe they know the answer and they are willing to share it with the rest of us. In this essay, I will explore the meaning of life from their perspective'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_lm.compile(sampler=\"top_k\")\n",
    "gemma_lm.generate(\"What is the meaning of life?\", max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a05c2",
   "metadata": {
    "id": "-okKgK4LfO0f",
    "papermill": {
     "duration": 0.010829,
     "end_time": "2024-02-21T12:01:37.435656",
     "exception": false,
     "start_time": "2024-02-21T12:01:37.424827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "While the default greedy algorithm always picks the token with the largest probability, the top-K algorithm randomly picks the next token from the tokens of top K probability.\n",
    "\n",
    "You don't have to specify a sampler, and you can ignore the last code snippet if it's not helpful to your use case. If you'd like learn more about the available samplers, see [Samplers](https://keras.io/api/keras_nlp/samplers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eabe881",
   "metadata": {
    "id": "jBrbTYasoo-J",
    "papermill": {
     "duration": 0.010681,
     "end_time": "2024-02-21T12:01:37.457471",
     "exception": false,
     "start_time": "2024-02-21T12:01:37.446790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What's next\n",
    "\n",
    "In this tutorial, you learned how to generate text using KerasNLP and Gemma. Here are a few suggestions for what to learn next:\n",
    "\n",
    "* Learn how to [finetune a Gemma model](https://ai.google.dev/gemma/docs/lora_tuning).\n",
    "* Learn how to perform [distributed fine-tuning and inference on a Gemma model](https://ai.google.dev/gemma/docs/distributed_tuning).\n",
    "* Learn how to [use Gemma models with Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "modelInstanceId": 5171,
     "sourceId": 10260,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1032.083032,
   "end_time": "2024-02-21T12:01:40.522757",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T11:44:28.439725",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
